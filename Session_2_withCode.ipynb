{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3e89c5b3",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/tapi-logo-small.png\" />\n",
    "\n",
    "This notebook free for educational reuse under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/).\n",
    "\n",
    "Created by [J.D. Porter](https://) for the 2023 Text Analysis Pedagogy Institute, with support from [Constellate](https://constellate.org).\n",
    "\n",
    "For questions/comments/improvements, email porterjd@upenn.sas.edu<br />\n",
    "____"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68f932d1",
   "metadata": {},
   "source": [
    "# `Finding Word Meaning Through Context` `2`\n",
    "\n",
    "This is lesson `2` of 3 in the educational series on `finding word meaning in context`. This notebook is intended `to teach some methods for finding the words that distingish one text from another, focusing especially on a Most Distinctive Words that uses a Fisher's Exact test`. \n",
    "\n",
    "**Audience:** `Teachers` / `Learners` / `Researchers`\n",
    "\n",
    "**Use case:** `Tutorial` \n",
    "\n",
    "**Difficulty:** `Beginner`\n",
    "\n",
    "**Completion time:** `90 minutes`\n",
    "\n",
    "**Knowledge Required:** \n",
    "```\n",
    "* Python basics (variables, functions, lists, dictionaries)\n",
    "* The content from Session 1:\n",
    "    * Open a text file\n",
    "    * Turn its text into a list of words\n",
    "    * Clean up the words a little (remove punctuation, change the case, etc.)\n",
    "    * Count the occurrences of each word\n",
    "\n",
    "```\n",
    "\n",
    "**Knowledge Recommended:**\n",
    "```\n",
    "* n/a\n",
    "```\n",
    "\n",
    "**Learning Objectives:**\n",
    "After this lesson, learners will be able to:\n",
    "```\n",
    "1. Understand the basic ideas of distinctive differences between texts\n",
    "2. Choose parameters for running a \"most distinctive words\" function\n",
    "3. Analyze the results of a \"most distinctive words\" check\n",
    "\n",
    "```\n",
    "**Research Pipeline:**\n",
    "```\n",
    "1. Gather some files, and perhaps also a metadata table\n",
    "2. Compare the word frequencies, and the statistical significance of their variance, in those files\n",
    "3. If you have written out some of your data, explore it in a program like Excel or Google Sheets\n",
    "4. Interpret!\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157c0555",
   "metadata": {},
   "source": [
    "# Required Python Libraries\n",
    "\n",
    "* To keep things simple, we will try to work with very few libraries in this notebook. This time we'll be using:\n",
    "    * os (to work with files and directories)\n",
    "    * scipy.stats (to measure statistical significance for some of our results)\n",
    "\n",
    "## Install Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5480e2a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:48.908106Z",
     "iopub.status.busy": "2023-07-19T20:48:48.907536Z",
     "iopub.status.idle": "2023-07-19T20:48:49.502866Z",
     "shell.execute_reply": "2023-07-19T20:48:49.502322Z",
     "shell.execute_reply.started": "2023-07-19T20:48:48.908085Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Import Libraries ###\n",
    "import os\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dedd148",
   "metadata": {},
   "source": [
    "# Required Data\n",
    "\n",
    "**Data Format:** \n",
    "* plain text (.txt)\n",
    "* maybe basic spreadsheet (.tsv)\n",
    "\n",
    "**Data Source:**\n",
    "\n",
    "Included files (though you may supplement these whenever you feel comfortable doing so!)\n",
    "\n",
    "**Data Quality/Bias:**\n",
    "\n",
    "Today's texts come from Wikipedia. They are subject to the usual biases associated with Wikipedia editors. I downloaded them on July 13, 2023, so they probably won't look exactly the same if you visit the site.\n",
    "\n",
    "**Data Description:**\n",
    "\n",
    "This lesson uses textual data in .txt format (utf-8 encoding) from Wikipedia."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53edaa2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Introduction to Session 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1851bbf-1e24-419a-981d-f77e93543358",
   "metadata": {},
   "source": [
    "Today's session might feel like a bit of a detour, but I want to cover this material today for a few reasons. \n",
    "\n",
    "First, we're going to need it at some point. The skills we learned in Session 1 have put us pretty close to being able to extract collocates—the context words that, we suspect, may help us interpret the meanings of words that interest us (let's call those 'target words'). But when we've found our collocates, how will we know which ones matter? After all, for any given target word, there will surely be many collocates, some of them the workaday words that tend to show up everywhere (\"the\",\"and\",\"of\", etc.), and, on the other end of the spectrum, some of them so infrequent that we might hesitate to ascribe much importance to them. To make sense of our data, we're going to want to be able to find words that are showing up _significantly_ more often near our target words.\n",
    "\n",
    "Second, this material is more complicated than most of what we're doing. On one hand, it might be a little bit of a challenge to jump right into this stuff, especially for those of you who are newer to programming or to Python. On the other hand, if we need to slow down, we can pick up the slack in Session 3!\n",
    "\n",
    "The last reason is more rhetorical. This material takes a little time to cover. By doing it now, we ensure that we get to collocates in the last session. It just feels more fun to _arrive_ at collocates on the last day, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb84723-deb5-488f-8504-71721c8d78ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Review: Getting word counts from texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8966f233-ad26-4ff1-a9c3-034ed740d4ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:50.963122Z",
     "iopub.status.busy": "2023-07-19T20:48:50.962484Z",
     "iopub.status.idle": "2023-07-19T20:48:50.965564Z",
     "shell.execute_reply": "2023-07-19T20:48:50.965063Z",
     "shell.execute_reply.started": "2023-07-19T20:48:50.963101Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start with a filename\n",
    "fn = \"Chopin_Awakening.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbadb315-91ee-4729-b75b-0b175b70ab3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:51.218771Z",
     "iopub.status.busy": "2023-07-19T20:48:51.218452Z",
     "iopub.status.idle": "2023-07-19T20:48:51.224400Z",
     "shell.execute_reply": "2023-07-19T20:48:51.223889Z",
     "shell.execute_reply.started": "2023-07-19T20:48:51.218750Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Open it and get the text\n",
    "with open(fn) as source:\n",
    "    text = source.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2ceb8ca4-743c-42f0-8869-9bfd7d159379",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:51.486694Z",
     "iopub.status.busy": "2023-07-19T20:48:51.486322Z",
     "iopub.status.idle": "2023-07-19T20:48:51.493973Z",
     "shell.execute_reply": "2023-07-19T20:48:51.493113Z",
     "shell.execute_reply.started": "2023-07-19T20:48:51.486657Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turn the text into words\n",
    "words = text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cec67461-2a5e-4259-8bb3-ea4840d315df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:51.715906Z",
     "iopub.status.busy": "2023-07-19T20:48:51.715585Z",
     "iopub.status.idle": "2023-07-19T20:48:51.720791Z",
     "shell.execute_reply": "2023-07-19T20:48:51.720252Z",
     "shell.execute_reply.started": "2023-07-19T20:48:51.715884Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Clean the words\n",
    "\n",
    "# We can use this function for today\n",
    "def alphaclean(someword):\n",
    "    chars = [i for i in someword if i.isalpha()]\n",
    "    cleanword = ''.join(chars)\n",
    "    cleanword = cleanword.lower()\n",
    "    return cleanword\n",
    "\n",
    "# Takes a filename and returns a list of words\n",
    "def file2words(somefilename,clean=True):\n",
    "    with open(somefilename) as source:\n",
    "        text = source.read()\n",
    "    words = text.split()\n",
    "    if clean:\n",
    "        words = [alphaclean(w) for w in words]\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f33406f-966c-400d-b8f2-234e7acfc793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:52.967434Z",
     "iopub.status.busy": "2023-07-19T20:48:52.967121Z",
     "iopub.status.idle": "2023-07-19T20:48:53.019013Z",
     "shell.execute_reply": "2023-07-19T20:48:53.018478Z",
     "shell.execute_reply.started": "2023-07-19T20:48:52.967412Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Count the words!\n",
    "counts = {}\n",
    "\n",
    "for w in words:\n",
    "    w = alphaclean(w)\n",
    "    if w not in counts:\n",
    "        counts[w] = 0\n",
    "    counts[w] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c29add9-d02b-4d50-a0cd-d2a49e6d1641",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:48:53.140398Z",
     "iopub.status.busy": "2023-07-19T20:48:53.140068Z",
     "iopub.status.idle": "2023-07-19T20:48:53.238120Z",
     "shell.execute_reply": "2023-07-19T20:48:53.237649Z",
     "shell.execute_reply.started": "2023-07-19T20:48:53.140373Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pride',\n",
       " 'and',\n",
       " 'prejudice',\n",
       " 'by',\n",
       " 'jane',\n",
       " 'austen',\n",
       " 'chapter',\n",
       " '',\n",
       " 'it',\n",
       " 'is',\n",
       " 'a',\n",
       " 'truth',\n",
       " 'universally',\n",
       " 'acknowledged',\n",
       " 'that',\n",
       " 'a',\n",
       " 'single',\n",
       " 'man',\n",
       " 'in',\n",
       " 'possession']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing out our file2words function\n",
    "\n",
    "fn = 'Austen_PrideAndPrejudice.txt'\n",
    "\n",
    "words = file2words(fn)\n",
    "\n",
    "words[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7bedf4-4d8f-4978-bfe9-51fba2d18a04",
   "metadata": {},
   "source": [
    "# Comparing files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95bfe8b6-f6a5-4300-b6d1-f791bfca198a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:49:00.589865Z",
     "iopub.status.busy": "2023-07-19T20:49:00.589540Z",
     "iopub.status.idle": "2023-07-19T20:49:00.594532Z",
     "shell.execute_reply": "2023-07-19T20:49:00.594036Z",
     "shell.execute_reply.started": "2023-07-19T20:49:00.589840Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BeatleTexts/Ringo_Starr.txt',\n",
       " 'BeatleTexts/John_Lennon.txt',\n",
       " 'BeatleTexts/Paul_McCartney.txt',\n",
       " 'BeatleTexts/George_Harrison.txt']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Getting the filenames from a directory\n",
    "\n",
    "# Make a variable for your \"source directory\"\n",
    "sdir = \"BeatleTexts\"\n",
    "\n",
    "files = os.listdir(sdir)\n",
    "\n",
    "# Make sure we only have the kind of files we want\n",
    "files = [i for i in files if i.endswith('txt')]\n",
    "\n",
    "# Create full path versions of our filenames\n",
    "files = [os.path.join(sdir,f) for f in files]\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5965e9b1-b007-4143-a3ce-fc56de23f221",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T18:01:17.306830Z",
     "iopub.status.busy": "2023-07-19T18:01:17.306511Z",
     "iopub.status.idle": "2023-07-19T18:01:17.356745Z",
     "shell.execute_reply": "2023-07-19T18:01:17.356253Z",
     "shell.execute_reply.started": "2023-07-19T18:01:17.306809Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009646032544875021"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seeing how long each file is\n",
    "\n",
    "wcs = {}\n",
    "\n",
    "beatle_counts = {}\n",
    "\n",
    "for f in files:\n",
    "    words = file2words(f)\n",
    "    for w in words:\n",
    "        if w not in beatle_counts:\n",
    "            beatle_counts[w] = 0\n",
    "        beatle_counts[w] += 1\n",
    "    wc = len(words)\n",
    "    wcs[f] = wc\n",
    "    \n",
    "total_wc = sum(beatle_counts.values())\n",
    "\n",
    "rates = {}\n",
    "\n",
    "for word,count in beatle_counts.items():\n",
    "    rates[word] = count / total_wc\n",
    "    \n",
    "rates['liverpool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9b8f557f-7e37-4bf0-b71f-9a2652b3a991",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T17:49:15.438831Z",
     "iopub.status.busy": "2023-07-19T17:49:15.438503Z",
     "iopub.status.idle": "2023-07-19T17:49:15.442580Z",
     "shell.execute_reply": "2023-07-19T17:49:15.442063Z",
     "shell.execute_reply.started": "2023-07-19T17:49:15.438809Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47688"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(wcs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "fd26d999-3691-4c7a-82e3-fa78496f669e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T17:37:25.407827Z",
     "iopub.status.busy": "2023-07-19T17:37:25.407506Z",
     "iopub.status.idle": "2023-07-19T17:37:25.445686Z",
     "shell.execute_reply": "2023-07-19T17:37:25.445144Z",
     "shell.execute_reply.started": "2023-07-19T17:37:25.407806Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BeatleTexts/Ringo_Starr.txt \t 10 \t 10.313531353135312\n",
      "BeatleTexts/John_Lennon.txt \t 11 \t 8.731544689633274\n",
      "BeatleTexts/Paul_McCartney.txt \t 20 \t 14.031149151115475\n",
      "BeatleTexts/George_Harrison.txt \t 5 \t 4.488330341113106\n"
     ]
    }
   ],
   "source": [
    "# Counting a target word\n",
    "target_word = 'liverpool'\n",
    "\n",
    "for f in files:\n",
    "    words = file2words(f)\n",
    "    count = words.count(target_word)\n",
    "    freq = count / len(words) * 10000\n",
    "    print(f,\"\\t\",count,\"\\t\",freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b4740a1-85fb-4089-9258-08724ae0e337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving from counts to frequencies\n",
    "# (We do this in the cell above - by adding the 'freq' variable!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0b395e-77d4-4729-89a5-530d8b3033ef",
   "metadata": {},
   "source": [
    "# Figuring out which counts _count_..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b900a797-25a1-4c44-b64b-5bcbec648ea6",
   "metadata": {},
   "source": [
    "Raw counts have pretty limited use when it comes to comparing a word across texts. They're not _nothing_: If someone uses a word a lot, they really did use the word a lot! Even if the text is long, you as the reader really did get exposed to all of those uses! But they don't tell us much about the _importance_ of a word in a text, especially if we want to compare across works of different lengths.\n",
    "\n",
    "Frequencies are a big improvement, since they scale for length. With frequencies, we can say that the Wikipedia article for Paul McCartney uses the word \"Liverpool\" more often _per word_ —that is, \"Liverpool\" tales up more of the text for McCartney than it does for the other lads. Yet even frequencies can be somewhat difficult to interpret. Just how big a difference is it to say \"Liverpool\" 14 times per 10,000 words compared to 10.3? Or to 8.7?\n",
    "\n",
    "There are several ways to approach this problem, and to my knowledge, none is exactly standard yet, at least in my corner of text analysis (Digital Humanities with a literary critical focus). You can find a nice summary of several well-established approaches on the [Zeta Project website](https://zeta-project.eu/en/keyness-measures/). You may have heard of some of these: chi-squared tests, TF-IDF scores, Burrow's Zeta... the list goes on.\n",
    "\n",
    "Although the approaches differ in important ways, they have in common the goal of finding _distinctive_ words by measuring which words show up in which places more often than we'd expect. More often? Expect? What do these terms mean? What exactly is our expectation based on? Well, it depends! We have so many methods precisely because of the ambiguity of this question, along with various practical considerations (e.g., some methods are better at finding \"content\" words, some are better at handling corpora of radically different sizes, and so on). \n",
    "\n",
    "The method I prefer was developed at the Stanford Literary Lab, and involves a modified form of a Fisher's Exact Test. For today, we'll focus on this method, for a couple reasons:\n",
    "* It's the method I know best, so I'm better positioned to discuss it.\n",
    "* To me the results are usually quite legible—this is one of those \"the proof is in the pudding\" situations where I've continued using this approach because I've liked the output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2054f3-2646-4cc5-8381-a5921ccbdc27",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fisher's Exact Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea2eea1-522b-4a54-af5c-1b12e2ab26e8",
   "metadata": {
    "tags": []
   },
   "source": [
    "Fisher's Exact Test was developed by Ronald Fisher when a colleague of his, Muriel Bristol, claimed that she could tell whether milk was added to a tea cup before or after the tea was poured. Fisher tested her with eight cups and she got them all right. In a roundabout way, this led Fisher to develop a test that could measure the statistical significance of results for experiments like this. (Everyone who teaches the Fisher's Exact Test is required to share this story.)\n",
    "\n",
    "\n",
    "The math behind this test is complicated, so we won't go into much detail here. If you're interested, I think [Wolfram MathWorld](https://mathworld.wolfram.com/FishersExactTest.html) has a pretty approachable explanation. They give the example of trying to figure out whether math and biology papers are appearing significantly more often in _Mathematics Magazine_ and _Science_ (respectively). The upshot is that running a Fisher's Exact Test requires creating a 2x2 grid, which in Python winds up being a list of lists (this is the same principle behind our approach to making an output table in Session 1). It looks roughly like this:\n",
    "```Python\n",
    "grid = [[a,b],[c,d]]\n",
    "```\n",
    "which, if you printed each row, would be:\n",
    "```Python\n",
    "[a,b]\n",
    "[c,d]\n",
    "```\n",
    "\n",
    "Figuring out what we need in order to fill in our variables (a,b,c, & d) will give us a decent frame for understanding what sort of information the test measures, and what its results mean for us. In the example above, from Wolfram MathWorld:\n",
    "\n",
    "* a = number of math papers in _Math Magazine_\n",
    "* b = number of math papers in _Science_\n",
    "* c = number of biology papers in _Math Magazine_\n",
    "* d = number of biology papers in _Science_\n",
    "\n",
    "For our purposes in text analysis, I'll be following an approach laid out in several Stanford Literary Lab pamphlets, perhaps most notably [\"Style at the Scale of the Sentence\"](https://litlab.stanford.edu/LiteraryLabPamphlet5.pdf).[$^{1}$](#1) It's a little unusual compared to the example above, but essentially it's trying to get at the question: Does the target word appear more often in one corpus than another? So to that end:\n",
    "\n",
    "* a = the number of times the word appeared in the corpus we're interested in\n",
    "* b = the number of times any other word appeared in that corpus\n",
    "* c = the number of times we would have expected the word to appear if it was evenly distributed across all our corpora\n",
    "* d = the number of times any other word would have appeared if the target word had been appearing at its expected rate\n",
    "\n",
    "Three of these values are pretty easy to find; only **c** is a little difficult. To get **c**, we need some kind of expected rate of appearance for our target word. My usual approach is to take the total appearances of the target word across all corpora and divide by the total word count of all the corpora. That's basically getting us the general frequency of the word. The idea is that if the target word were evenly distributed, then we'd expect it to appear at the same rate _per-word_ in every corpus. Let's call the rate **r**. So, for our Beatles example above, \"liverpool\" appears 46 times, and the total wordcount of all four articles is 47,688. To get **r**, we would just do:\n",
    "\n",
    "```Python\n",
    "r =  46 / 47,688\n",
    "```\n",
    "Or in other words, **r** is about **.00096**. Once we have our **r**, it becomes easy to get **c**, which means all of our variables are now pretty easy. Let's see if Paul's article really does mention Liverpool _significantly_ more often than the other articles. And just as a reminder, Paul's article has a total wordcount of 14,254 (let's call that **wc**):\n",
    "* a = The number of times Paul's article says \"liverpool\"\n",
    "* b = wc - a\n",
    "* c = r * wc (we actually round this number to make the future calculations work)\n",
    "* d = wc - c\n",
    "\n",
    "Thus:\n",
    "\n",
    "```Python\n",
    "a = 20\n",
    "b = 14234\n",
    "c = 14\n",
    "d = 14240\n",
    "```\n",
    "\n",
    "At this point, the complex calculation happens. But fortunately for us, Python makes this much easier. This brings us to the end of this wall of text, because now we're going to run some actual code!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##### *Footnotes* #####\n",
    "\n",
    "1. <a id=\"1\"></a> Allison, Sarah, Marissa Gemma, Ryan Heuser, Franco Moretti, Amir Tevel, Irena Yamboliev. \"Style at the Scale of the Sentence\". Stanford Literary Lab pamphlet series, no. 5.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a8ce1192-3c7c-4539-ac70-52056ffb9be9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T17:56:14.716709Z",
     "iopub.status.busy": "2023-07-19T17:56:14.716169Z",
     "iopub.status.idle": "2023-07-19T17:56:14.732493Z",
     "shell.execute_reply": "2023-07-19T17:56:14.731968Z",
     "shell.execute_reply.started": "2023-07-19T17:56:14.716689Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1956219509287667"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Running the scipy.stats fisher_exact test on our \"liverpool\" data\n",
    "\n",
    "target_word = 'liverpool'\n",
    "\n",
    "words = file2words('BeatleTexts/Paul_McCartney.txt')\n",
    "\n",
    "count = words.count(target_word)\n",
    "\n",
    "wc = len(words)\n",
    "\n",
    "r =  46 / 47688\n",
    "\n",
    "a = count\n",
    "b = wc - a\n",
    "c = round(r * wc)\n",
    "d = wc-c\n",
    "\n",
    "table = [[a,b],[c,d]]\n",
    "\n",
    "# Run the Fisher's Exact Test\n",
    "fe = fisher_exact(table,alternative='greater')\n",
    "\n",
    "# Get the p-value\n",
    "fe.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff68aa14-43bc-4318-a0e0-a4356dba6c57",
   "metadata": {},
   "source": [
    "In this case the resulting p-value is about .2. \n",
    "\n",
    "We won't go into p-values very much in this session, but the general idea (at least as we're using it) is that a p-value captures the likelihood that your result might have occurred by chance. A lower p-value indicates that the result probably didn't occur randomly, or in other words that our hypothesized explanation is more apt to be a _good_ explanation.\n",
    "\n",
    "Generally speaking, people set their p-value cutoff (often called _alpha_) at .05. Anything below that is considered \"statistically significant\". There is no hard and fast reason why alpha needs to be .05, and some have argued that in some cases it should be much lower. Still, as far as I'm aware, .05 remains a fairly standard alpha.\n",
    "\n",
    "The nuances of proper alpha thresholds don't even matter in this case, though: Our p-value for \"liverpool\" in Paul's article is _well_ above alpha! We can be pretty confident that Paul's article _doesn't_ meaningfully say \"liverpool\" more than those for the other lads.\n",
    "\n",
    "Let's try another word for practice: \"bass\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "76957884-b873-4510-b016-f91d922bd36a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T18:04:31.540996Z",
     "iopub.status.busy": "2023-07-19T18:04:31.540319Z",
     "iopub.status.idle": "2023-07-19T18:04:31.553624Z",
     "shell.execute_reply": "2023-07-19T18:04:31.553135Z",
     "shell.execute_reply.started": "2023-07-19T18:04:31.540964Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.059163723617446214"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's copy some of our \"liverpool\" code from above and switch the target word\n",
    "target_word = 'drums'\n",
    "\n",
    "words = file2words('BeatleTexts/Ringo_Starr.txt')\n",
    "\n",
    "count = words.count(target_word)\n",
    "\n",
    "wc = len(words)\n",
    "\n",
    "r = rates[target_word]\n",
    "\n",
    "a = count\n",
    "b = wc - a\n",
    "c = round(r * wc)\n",
    "d = wc-c\n",
    "\n",
    "table = [[a,b],[c,d]]\n",
    "\n",
    "# Run the Fisher's Exact Test\n",
    "fe = fisher_exact(table,alternative='greater')\n",
    "\n",
    "# Get the p-value\n",
    "fe.pvalue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23d7ac7-613c-4685-8740-3161fdedb2b7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finding all of the distinctive words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ce580f-afe5-4e68-8504-a3f517b5bc79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:49:52.562835Z",
     "iopub.status.busy": "2023-07-19T20:49:52.562252Z",
     "iopub.status.idle": "2023-07-19T20:49:52.566219Z",
     "shell.execute_reply": "2023-07-19T20:49:52.565731Z",
     "shell.execute_reply.started": "2023-07-19T20:49:52.562808Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, let's go through this function that will get the p-value for a target word and figure out what we need\n",
    "\n",
    "def get_fishers(someword,somecountdict,someratedict,alternative='greater'):\n",
    "    r = someratedict[someword]\n",
    "    wc = sum(somecountdict.values())\n",
    "    a = somecountdict[someword]\n",
    "    b = wc - a\n",
    "    c = round(r*wc)\n",
    "    d = wc-c\n",
    "    p = fisher_exact([[a,b],[c,d]],alternative=alternative).pvalue\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a21a3b4-5549-433a-b6c7-f4660e5db572",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'She said, \"Hello\"'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b6e936e-b135-4638-8a7e-cdbde73537e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:49:58.361597Z",
     "iopub.status.busy": "2023-07-19T20:49:58.361025Z",
     "iopub.status.idle": "2023-07-19T20:49:58.425417Z",
     "shell.execute_reply": "2023-07-19T20:49:58.424933Z",
     "shell.execute_reply.started": "2023-07-19T20:49:58.361573Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0009646032544875021"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To make life easier later, we may want to make a nested dictionary with results for every subcorpus (here, just documents)\n",
    "\n",
    "doc_counts = {}\n",
    "\n",
    "all_counts = {}\n",
    "\n",
    "rates = {}\n",
    "\n",
    "for f in files:\n",
    "    doc_counts[f] = {}\n",
    "    words = file2words(f)\n",
    "    for w in words:\n",
    "        if w not in doc_counts[f]:\n",
    "            doc_counts[f][w] = 0\n",
    "        doc_counts[f][w] += 1\n",
    "        if w not in all_counts:\n",
    "            all_counts[w] = 0\n",
    "        all_counts[w] += 1\n",
    "\n",
    "total_wc = sum(all_counts.values())\n",
    "\n",
    "for word,count in all_counts.items():\n",
    "    rates[word] = count / total_wc\n",
    "        \n",
    "# As we go, if we record the overall count of each word, it will be easier to make a rate dictionary later\n",
    "\n",
    "        \n",
    "# Now we can make a rate dictionary using the info we've already collected\n",
    "\n",
    "rates['liverpool']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ba05b7-5f73-44ca-9fb9-e97315e67d63",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make some decisions\n",
    "\n",
    "* Pick a cutoff (words must appear x times or we ignore them)\n",
    "    * A higher cutoff makes the process much faster\n",
    "    * You may also find infrequent words less interesting to analyze (although maybe not!)\n",
    "* Choose an alpha (.05 is standard)\n",
    "* Decide whether you want to exclude stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f76e1d21-4949-45a8-9a51-14a7d2e4d5ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:50:02.487900Z",
     "iopub.status.busy": "2023-07-19T20:50:02.487527Z",
     "iopub.status.idle": "2023-07-19T20:50:02.490813Z",
     "shell.execute_reply": "2023-07-19T20:50:02.490292Z",
     "shell.execute_reply.started": "2023-07-19T20:50:02.487877Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "cutoff = 5\n",
    "exclude_stops = True\n",
    "alpha = .05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4345cb89-820f-4c95-bec0-58378e31fa5b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:50:02.603544Z",
     "iopub.status.busy": "2023-07-19T20:50:02.602963Z",
     "iopub.status.idle": "2023-07-19T20:50:02.606832Z",
     "shell.execute_reply": "2023-07-19T20:50:02.606337Z",
     "shell.execute_reply.started": "2023-07-19T20:50:02.603523Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get some stopwords\n",
    "stops = file2words('stopwords.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5f8f0a2-00b7-47c6-889d-f210429e1cbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T20:50:24.294696Z",
     "iopub.status.busy": "2023-07-19T20:50:24.294101Z",
     "iopub.status.idle": "2023-07-19T20:50:24.368234Z",
     "shell.execute_reply": "2023-07-19T20:50:24.367690Z",
     "shell.execute_reply.started": "2023-07-19T20:50:24.294675Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "output_table = [['token_','count','p-value','obs/exp']]\n",
    "\n",
    "# Change the filename in this line to get MDW for different Beatles!\n",
    "countdict = doc_counts['BeatleTexts/George_Harrison.txt']\n",
    "\n",
    "for word,count in countdict.items():\n",
    "    if count < cutoff:\n",
    "        continue\n",
    "    if word in stops:\n",
    "        continue\n",
    "    p = get_fishers(word,countdict,rates)\n",
    "    exp = rates[word] * sum(countdict.values())\n",
    "    if p < alpha:\n",
    "        new_row = [word,count,p,count/exp]\n",
    "        output_table.append(new_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "dc636674-7d8e-48ad-bec5-ed3dcdd956e1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-07-19T18:26:54.755094Z",
     "iopub.status.busy": "2023-07-19T18:26:54.754779Z",
     "iopub.status.idle": "2023-07-19T18:26:54.758860Z",
     "shell.execute_reply": "2023-07-19T18:26:54.758199Z",
     "shell.execute_reply.started": "2023-07-19T18:26:54.755074Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Write out your results here\n",
    "# Be careful to change the filename, or you'll write over what you've done before!\n",
    "\n",
    "with open('george_mdw.csv','w') as output:\n",
    "    for row in output_table:\n",
    "        str_list = [str(i) for i in row]\n",
    "        output_str = \",\".join(str_list) + \"\\n\"\n",
    "        output.write(output_str)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
